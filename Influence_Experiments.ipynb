{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZaSvcRfXyMeC"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.utils import data\n","from tqdm import tqdm\n","import copy\n","import time\n","import abc\n","from typing import Any, List, Optional, Callable\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","import os\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{"id":"3HksaKqGivtC"},"source":["# Code from influence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIZmQifVyV72"},"outputs":[],"source":["def _set_attr(obj, names, val):\n","    if len(names) == 1:\n","        setattr(obj, names[0], val)\n","    else:\n","        _set_attr(getattr(obj, names[0]), names[1:], val)\n","\n","\n","def _del_attr(obj, names):\n","    if len(names) == 1:\n","        delattr(obj, names[0])\n","    else:\n","        _del_attr(getattr(obj, names[0]), names[1:])\n","\n","class BaseObjective(abc.ABC):\n","    \"\"\"An abstract adapter that provides torch-influence with project-specific information\n","    about how training and test objectives are computed.\n","\n","    In order to use torch-influence in your project, a subclass of this module should be\n","    created that implements this module's four abstract methods.\n","    \"\"\"\n","\n","    @abc.abstractmethod\n","    def train_outputs(self, model: nn.Module, batch: Any) -> torch.Tensor:\n","        \"\"\"Returns a batch of model outputs (e.g., logits, probabilities) from a batch of data.\n","\n","        Args:\n","            model: the model.\n","            batch: a batch of training data.\n","\n","        Returns:\n","            the model outputs produced from the batch.\n","        \"\"\"\n","\n","        raise NotImplementedError()\n","\n","    @abc.abstractmethod\n","    def train_loss_on_outputs(self, outputs: torch.Tensor, batch: Any) -> torch.Tensor:\n","        \"\"\"Returns the **mean**-reduced loss of the model outputs produced from a batch of data.\n","\n","        Args:\n","            outputs: a batch of model outputs.\n","            batch: a batch of training data.\n","\n","        Returns:\n","            the loss of the outputs over the batch.\n","\n","        Note:\n","            There may be some ambiguity in how to define :meth:`train_outputs()` and\n","            :meth:`train_loss_on_outputs()`: what point in the forward pass deliniates\n","            outputs from loss function? For example, in binary classification, the\n","            outputs can reasonably be taken to be the model logits or normalized probabilities.\n","\n","            For standard use of influence functions, both choices produce the same behaviour.\n","            However, if using the Gauss-Newton Hessian approximation for influence functions,\n","            we require that :meth:`train_loss_on_outputs()` be convex in the model\n","            outputs.\n","\n","        See also:\n","            :class:`CGInfluenceModule`\n","            :class:`LiSSAInfluenceModule`\n","        \"\"\"\n","\n","        raise NotImplementedError()\n","\n","    @abc.abstractmethod\n","    def train_regularization(self, params: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Returns the regularization loss at a set of model parameters.\n","\n","        Args:\n","            params: a flattened vector of model parameters.\n","\n","        Returns:\n","            the regularization loss.\n","        \"\"\"\n","\n","        raise NotImplementedError()\n","\n","    def train_loss(self, model: nn.Module, params: torch.Tensor, batch: Any) -> torch.Tensor:\n","        \"\"\"Returns the **mean**-reduced regularized loss of a model over a batch of data.\n","\n","        This method should not be overridden for most use cases. By default, torch-influence\n","        takes and expects the overall training loss to be::\n","\n","            outputs = train_outputs(model, batch)\n","            loss = train_loss_on_outputs(outputs, batch) + train_regularization(params)\n","\n","        Args:\n","            model: the model.\n","            params: a flattened vector of the model's parameters.\n","            batch: a batch of training data.\n","\n","        Returns:\n","            the training loss over the batch.\n","        \"\"\"\n","\n","        outputs = self.train_outputs(model, batch)\n","        return self.train_loss_on_outputs(outputs, batch) + self.train_regularization(params)\n","\n","    @abc.abstractmethod\n","    def test_loss(self, model: nn.Module, params: torch.Tensor, batch: Any) -> torch.Tensor:\n","        \"\"\"Returns the **mean**-reduced loss of a model over a batch of data.\n","\n","        Args:\n","            model: the model.\n","            params: a flattened vector of the model's parameters.\n","            batch: a batch of test data.\n","\n","        Returns:\n","            the test loss over the batch.\n","        \"\"\"\n","\n","        raise NotImplementedError()\n","\n","\n","\n","class BaseInfluenceModule(abc.ABC):\n","    \"\"\"The core module that contains convenience methods for computing influence functions.\n","\n","    Args:\n","        model: the model of interest.\n","        objective: an implementation of :class:`BaseObjective`.\n","        train_loader: a training dataset loader.\n","        test_loader: a test dataset loader.\n","        device: the device on which operations are performed.\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            model: nn.Module,\n","            objective: BaseObjective,\n","            train_loader: data.DataLoader,\n","            test_loader: data.DataLoader,\n","            device: torch.device\n","    ):\n","        model.eval()\n","        self.model = model.to(device)\n","        self.device = device\n","\n","        self.is_model_functional = False\n","        self.params_names = tuple(name for name, _ in self._model_params())\n","        self.params_shape = tuple(p.shape for _, p in self._model_params())\n","\n","        self.objective = objective\n","        self.train_loader = train_loader\n","        self.test_loader = test_loader\n","\n","    @abc.abstractmethod\n","    def inverse_hvp(self, vec: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Computes an inverse-Hessian vector product, where the Hessian is specifically\n","        that of the (mean) empirical risk over the training dataset.\n","\n","        Args:\n","            vec: a vector.\n","\n","        Returns:\n","            the inverse-Hessian vector product.\n","        \"\"\"\n","\n","        raise NotImplementedError()\n","\n","    # ====================================================\n","    # Interface functions\n","    # ====================================================\n","\n","    def train_loss_grad(self, train_idxs: List[int]) -> torch.Tensor:\n","        \"\"\"Returns the gradient of the (mean) training loss over a set of training\n","        data points with respect to the model's flattened parameters.\n","\n","        Args:\n","            train_idxs: the indices of the training points.\n","\n","        Returns:\n","            the loss gradient at the training points.\n","        \"\"\"\n","\n","        return self._loss_grad(train_idxs, train=True)\n","\n","    def test_loss_grad(self, test_idxs: List[int]) -> torch.Tensor:\n","        \"\"\"Returns the gradient of the (mean) test loss over a set of test\n","        data points with respect to the model's flattened parameters.\n","\n","        Args:\n","           test_idxs: the indices of the test points.\n","\n","        Returns:\n","           the loss gradient at the test points.\n","        \"\"\"\n","\n","        return self._loss_grad(test_idxs, train=False)\n","\n","    def stest(self, test_idxs: List[int]) -> torch.Tensor:\n","        \"\"\"This function simply composes :func:`inverse_hvp` with :func:`test_loss_grad`.\n","\n","        In the original influence function paper, the resulting vector was called\n","        :math:`\\mathbf{s}_{\\mathrm{test}}`.\n","\n","        Args:\n","            test_idxs: the indices of the test points.\n","\n","        Returns:\n","            the :math:`\\mathbf{s}_{\\mathrm{test}}` vector.\n","        \"\"\"\n","\n","        return self.inverse_hvp(self.test_loss_grad(test_idxs))\n","\n","    def influences(\n","            self,\n","            train_idxs: List[int],\n","            test_idxs: List[int],\n","            stest: Optional[torch.Tensor] = None\n","    ) -> torch.Tensor:\n","        \"\"\"Returns the influence scores of a set of training data points with respect to\n","        the (mean) test loss over a set of test data points.\n","\n","        Specifically, this method returns a 1D tensor of ``len(train_idxs)`` influence scores.\n","        These scores estimate the following quantities:\n","\n","            Let :math:`\\mathcal{L}_0` be the (mean) test loss of the current model\n","            over the input test points. Suppose we produce a new model by (1) removing\n","            the ``train_idxs[i]``-th example from the training dataset and (2) retraining\n","            the model on this one-smaller dataset. Let :math:`\\mathcal{L}` be the (mean)\n","            test loss of the **new** model over the input test points. Then the ``i``-th\n","            influence score estimates :math:`\\mathcal{L} - \\mathcal{L}_0`.\n","\n","        Args:\n","            train_idxs: the indices of the training points.\n","            test_idxs: the indices of the test points.\n","            stest: this method requires the :math:`\\mathbf{s}_{\\mathrm{test}}` vector of\n","                the input test points. If not ``None``, this argument will be used taken as\n","                :math:`\\mathbf{s}_{\\mathrm{test}}`. Otherwise, :math:`\\mathbf{s}_{\\mathrm{test}}`\n","                will be computed internally with :meth:`stest`.\n","\n","        Returns:\n","            the influence scores.\n","        \"\"\"\n","\n","        stest = self.stest(test_idxs) if (stest is None) else stest.to(self.device)\n","\n","        scores = []\n","        for grad_z, _ in self._loss_grad_loader_wrapper(batch_size=1, subset=train_idxs, train=True):\n","            s = grad_z @ stest\n","            scores.append(s)\n","        return torch.tensor(scores) / len(self.train_loader.dataset)\n","\n","    def unlearning(\n","            self,\n","            train_idxs: List[int]\n","    ) -> torch.Tensor:\n","        \"\"\"Unlearns pre-specified training samples from a trained model .\n","\n","        Returns:\n","            the unlearned model\n","        \"\"\"\n","        curr_vec = self.inverse_hvp(self.train_loss_grad(train_idxs), train_len = len(self.train_loader.dataset), unlearning=True)\n","        return self.model\n","\n","    # ====================================================\n","    # Private helper functions\n","    # ====================================================\n","\n","    # Model and parameter helpers\n","\n","    def _model_params(self, with_names=True):\n","        assert not self.is_model_functional\n","        return tuple((name, p) if with_names else p for name, p in self.model.named_parameters() if p.requires_grad)\n","\n","    def _model_make_functional(self):\n","        assert not self.is_model_functional\n","        params = tuple(p.detach().requires_grad_() for p in self._model_params(False))\n","\n","        for name in self.params_names:\n","            _del_attr(self.model, name.split(\".\"))\n","        self.is_model_functional = True\n","\n","        return params\n","\n","    def _model_reinsert_params(self, params, register=False):\n","        for name, p in zip(self.params_names, params):\n","            _set_attr(self.model, name.split(\".\"), torch.nn.Parameter(p) if register else p)\n","        self.is_model_functional = not register\n","\n","    def _flatten_params_like(self, params_like):\n","        vec = []\n","        for p in params_like:\n","            vec.append(p.view(-1))\n","        return torch.cat(vec)\n","\n","    def _reshape_like_params(self, vec):\n","        pointer = 0\n","        split_tensors = []\n","        for dim in self.params_shape:\n","            num_param = dim.numel()\n","            split_tensors.append(vec[pointer: pointer + num_param].view(dim))\n","            pointer += num_param\n","        return tuple(split_tensors)\n","\n","    # Data helpers\n","\n","    def _transfer_to_device(self, batch):\n","        if isinstance(batch, torch.Tensor):\n","            return batch.to(self.device)\n","        elif isinstance(batch, (tuple, list)):\n","            return type(batch)(self._transfer_to_device(x) for x in batch)\n","        elif isinstance(batch, dict):\n","            return {k: self._transfer_to_device(x) for k, x in batch.items()}\n","        else:\n","            raise NotImplementedError()\n","\n","    def _loader_wrapper(self, train, batch_size=None, subset=None, sample_n_batches=-1):\n","        loader = self.train_loader if train else self.test_loader\n","        batch_size = loader.batch_size if (batch_size is None) else batch_size\n","\n","        if subset is None:\n","            dataset = loader.dataset\n","        else:\n","            subset = np.array(subset)\n","            if len(subset.shape) != 1 or len(np.unique(subset)) != len(subset):\n","                raise ValueError()\n","            if np.any((subset < 0) | (subset >= len(loader.dataset))):\n","                raise IndexError()\n","            dataset = data.Subset(loader.dataset, indices=subset)\n","\n","        if sample_n_batches > 0:\n","            num_samples = sample_n_batches * batch_size\n","            sampler = data.RandomSampler(data_source=dataset, replacement=True, num_samples=num_samples)\n","        else:\n","            sampler = None\n","\n","        new_loader = data.DataLoader(\n","            dataset=dataset,\n","            batch_size=batch_size,\n","            shuffle=False,\n","            sampler=sampler,\n","            collate_fn=loader.collate_fn,\n","            num_workers=loader.num_workers,\n","            worker_init_fn=loader.worker_init_fn,\n","        )\n","\n","        data_left = len(dataset)\n","        for batch in new_loader:\n","            batch = self._transfer_to_device(batch)\n","            size = min(batch_size, data_left)  # deduce batch size\n","            yield batch, size\n","            data_left -= size\n","\n","    # Loss and autograd helpers\n","\n","    def _loss_grad_loader_wrapper(self, train, **kwargs):\n","        params = self._model_params(with_names=False)\n","        flat_params = self._flatten_params_like(params)\n","\n","        for batch, batch_size in self._loader_wrapper(train=train, **kwargs):\n","            loss_fn = self.objective.train_loss if train else self.objective.test_loss\n","            loss = loss_fn(model=self.model, params=flat_params, batch=batch)\n","            yield self._flatten_params_like(torch.autograd.grad(loss, params)), batch_size\n","\n","    def _loss_grad(self, idxs, train):\n","        grad = 0.0\n","        for grad_batch, batch_size in self._loss_grad_loader_wrapper(subset=idxs, train=train):\n","            grad = grad + grad_batch * batch_size\n","        return grad / len(idxs)\n","\n","    def _hvp_at_batch(self, batch, flat_params, vec, gnh):\n","\n","        def f(theta_):\n","            self._model_reinsert_params(self._reshape_like_params(theta_))\n","            return self.objective.train_loss(self.model, theta_, batch)\n","\n","        def out_f(theta_):\n","            self._model_reinsert_params(self._reshape_like_params(theta_))\n","            return self.objective.train_outputs(self.model, batch)\n","\n","        def loss_f(out_):\n","            return self.objective.train_loss_on_outputs(out_, batch)\n","\n","        def reg_f(theta_):\n","            return self.objective.train_regularization(theta_)\n","\n","        if gnh:\n","            y, jvp = torch.autograd.functional.jvp(out_f, flat_params, v=vec)\n","            hjvp = torch.autograd.functional.hvp(loss_f, y, v=jvp)[1]\n","            gnhvp_batch = torch.autograd.functional.vjp(out_f, flat_params, v=hjvp)[1]\n","            return gnhvp_batch + torch.autograd.functional.hvp(reg_f, flat_params, v=vec)[1]\n","        else:\n","            return torch.autograd.functional.hvp(f, flat_params, v=vec)[1]\n","\n","\n","class LiSSAInfluenceModule(BaseInfluenceModule):\n","    r\"\"\"An influence module that computes inverse-Hessian vector products\n","    using the Linear time Stochastic Second-Order Algorithm (LiSSA).\n","\n","    At a high level, LiSSA estimates an inverse-Hessian vector product\n","    by using truncated Neumann iterations:\n","\n","    .. math::\n","        \\mathbf{H}^{-1}\\mathbf{v} \\approx \\frac{1}{R}\\sum\\limits_{r = 1}^R\n","        \\left(\\sigma^{-1}\\sum_{t = 1}^{T}(\\mathbf{I} - \\sigma^{-1}\\mathbf{H}_{r, t})^t\\mathbf{v}\\right)\n","\n","    Here, :math:`\\mathbf{H}` is the risk Hessian matrix and :math:`\\mathbf{H}_{r, t}` are\n","    loss Hessian matrices over batches of training data drawn randomly with replacement (we\n","    also use a batch size in ``train_loader``). In addition, :math:`\\sigma > 0` is a scaling\n","    factor chosen sufficiently large such that :math:`\\sigma^{-1} \\mathbf{H} \\preceq \\mathbf{I}`.\n","\n","    In practice, we can compute each inner sum recursively. Starting with\n","    :math:`\\mathbf{h}_{r, 0} = \\mathbf{v}`, we can iteratively update for :math:`T` steps:\n","\n","    .. math::\n","        \\mathbf{h}_{r, t} = \\mathbf{v} + \\mathbf{h}_{r, t - 1} - \\sigma^{-1}\\mathbf{H}_{r, t}\\mathbf{h}_{r, t - 1}\n","\n","    where :math:`\\mathbf{h}_{r, T}` will be equal to the :math:`r`-th inner sum.\n","\n","    Args:\n","        model: the model of interest.\n","        objective: an implementation of :class:`BaseObjective`.\n","        train_loader: a training dataset loader.\n","        test_loader: a test dataset loader.\n","        device: the device on which operations are performed.\n","        damp: the damping strength :math:`\\lambda`. Influence functions assume that the\n","            risk Hessian :math:`\\mathbf{H}` is positive-definite, which often fails to\n","            hold for neural networks. Hence, a damped risk Hessian :math:`\\mathbf{H} + \\lambda\\mathbf{I}`\n","            is used instead, for some sufficiently large :math:`\\lambda > 0` and\n","            identity matrix :math:`\\mathbf{I}`.\n","        repeat: the number of trials :math:`R`.\n","        depth: the recurrence depth :math:`T`.\n","        scale: the scaling factor :math:`\\sigma`.\n","        gnh: if ``True``, the risk Hessian :math:`\\mathbf{H}` is approximated with\n","            the Gauss-Newton Hessian, which is positive semi-definite.\n","            Otherwise, the risk Hessian is used.\n","        debug_callback: a callback function which is passed in :math:`(r, t, \\mathbf{h}_{r, t})`\n","            at each recurrence step.\n","     \"\"\"\n","\n","    def __init__(\n","            self,\n","            model: nn.Module,\n","            objective: BaseObjective,\n","            train_loader: data.DataLoader,\n","            test_loader: data.DataLoader,\n","            device: torch.device,\n","            damp: float,\n","            repeat: int,\n","            depth: int,\n","            scale: float,\n","            gnh: bool = False,\n","            debug_callback: Optional[Callable[[int, int, torch.Tensor], None]] = None\n","    ):\n","\n","        super().__init__(\n","            model=model,\n","            objective=objective,\n","            train_loader=train_loader,\n","            test_loader=test_loader,\n","            device=device,\n","        )\n","\n","        self.damp = damp\n","        self.gnh = gnh\n","        self.repeat = repeat\n","        self.depth = depth\n","        self.scale = scale\n","        self.debug_callback = debug_callback\n","\n","    def inverse_hvp(self, vec, unlearning=False):\n","\n","        params = self._model_make_functional()\n","        flat_params = self._flatten_params_like(params)\n","\n","        ihvp = 0.0\n","\n","        for r in range(self.repeat):\n","\n","            h_est = vec.clone()\n","\n","            for t, (batch, _) in enumerate(self._loader_wrapper(sample_n_batches=self.depth, train=True)):\n","\n","                hvp_batch = self._hvp_at_batch(batch, flat_params, vec=h_est, gnh=self.gnh)\n","\n","                with torch.no_grad():\n","                    hvp_batch = hvp_batch + self.damp * h_est\n","                    h_est = vec + h_est - hvp_batch / self.scale\n","\n","                if self.debug_callback is not None:\n","                    self.debug_callback(r, t, h_est)\n","\n","            ihvp = ihvp + h_est / self.scale\n","\n","        with torch.no_grad():\n","            self._model_reinsert_params(self._reshape_like_params(flat_params), register=True)\n","\n","        return ihvp / self.repeat\n"]},{"cell_type":"markdown","metadata":{"id":"H7nEyNktyct_"},"source":["Common Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJA7fTs2yeTL"},"outputs":[],"source":["L2_WEIGHT = 1e-6 #1e-4\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","###########################################\n","# train resnet\n","def train_model(model, train_loader, test_loader, DEVICE, lr = 0.001, num_epochs = 10, L2_WEIGHT= 1e-6):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=L2_WEIGHT)\n","\n","    # Fine-tuning the model\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for images, labels in tqdm(train_loader, leave=False):\n","            images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n","\n","        # Evaluate the model on the test set\n","        model.eval()\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(DEVICE), labels.to(DEVICE)\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n","    return model\n","\n","###########################################\n","# calculate test losses\n","def return_test_losses(test_loader, trained_model):\n","    test_losses = []\n","    trained_model.eval()\n","    with torch.no_grad():\n","        for images, labels in test_loader:  # Assuming test_loader is defined\n","            images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","            # Forward pass to get model outputs\n","            outputs = trained_model(images)\n","\n","            # Compute losses with reduction set to 'none' to get losses per image\n","            losses = nn.CrossEntropyLoss(reduction='none')(outputs, labels)\n","            test_losses.append(losses.cpu())  # Store losses\n","        # Concatenate all losses into a single tensor\n","        test_losses_tensor = torch.cat(test_losses)\n","\n","    return test_losses_tensor\n","\n","###########################################\n","# caption CIFAR10 images\n","def load_cifar10_examples(split, desired_classes, idx):\n","    \"\"\"Load CIFAR-10 examples based on the split and index.\"\"\"\n","    if split == \"train\":\n","        dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n","    else:\n","        dataset = datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","    indices = [i for i, (_, label) in enumerate(dataset) if label in desired_classes]\n","    filtered_train_dataset = data.Subset(dataset, indices)\n","\n","    image, label = filtered_train_dataset[idx]\n","    return image, label\n","\n","def captioned_image(model, datasample, split, idx, scores, desired_classes, device):\n","\n","    y_hat = model(datasample[0].view(1, datasample[0].shape[0], datasample[0].shape[1], datasample[0].shape[2]).to(device)).argmax(dim=1).item()  # Use argmax for multi-class prediction\n","\n","    # turn image into [0 .. 255] RGB image\n","    image, _ = load_cifar10_examples(split, desired_classes, idx)\n","    image = np.array(image)\n","\n","    # turn labels into human-readable strings\n","    class_names = [\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n","    label = class_names[int(datasample[1])]\n","    pred = class_names[int(y_hat)]\n","\n","    if split == \"test\":\n","        score_caption = f\"Test Loss: {scores[idx]:.5f}\"\n","    else:\n","        score_caption = f\"Influence: {scores[idx]:+.5f}\"\n","    label_caption = f\"Pred: {pred}, Label: {label}\"\n","    return image, label_caption + \"\\n\" + score_caption\n"]},{"cell_type":"markdown","metadata":{"id":"PMYeL-K6yy97"},"source":["# Main Code"]},{"cell_type":"markdown","metadata":{"id":"XE089zwT3oQK"},"source":["Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJb3R2gD3nwG"},"outputs":[],"source":["print(f\"Using {DEVICE}\")\n","\n","# ===========\n","# Load model and data\n","# ==========\n","batch_size = 128\n","# Data transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize to match ResNet input size\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n","])\n","# we take only two classes out of the CIFAR10 data set\n","desired_classes = [0, 1]  # Change to the class indices you want\n","\n","# Load CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","indices = [i for i, (_, label) in enumerate(train_dataset) if label in desired_classes]\n","filtered_train_dataset = data.Subset(train_dataset, indices)\n","print('Loaded 1')\n","\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","indices = [i for i, (_, label) in enumerate(test_dataset) if label in desired_classes]\n","filtered_test_dataset = data.Subset(test_dataset, indices)\n","print('Loaded 2')\n","\n","train_loader = data.DataLoader(dataset=filtered_train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(dataset=filtered_test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LGc1mjbmy0iV"},"outputs":[],"source":["# load pre-trained model and replace its head\n","model_type = 'ResNet18'\n","if model_type == 'VGG13':\n","    model = models.vgg13(weights=\"IMAGENET1K_V1\")\n","    model.classifier[6] = nn.Linear(4096, len(desired_classes))\n","    model.to(DEVICE)\n","else:\n","    model = models.resnet18(weights=\"IMAGENET1K_V1\")\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, len(desired_classes)) # 10 classes in CIFAR-10\n","    model.to(DEVICE)\n","\n","# initialize hyperparameters\n","lr = 0.00001\n","num_epochs = 50\n","num_test_points = 30\n","num_test_point_plot = 3\n","\n","# lissa hyperparameters\n","repeat_lissa = 5\n","depth_lissa = 1000\n","scale_lissa = 500\n","model_path = os.path.join(os.getcwd(), model_type, '_trained_cifar10_lr_' + str(lr) + '_epochs_' + str(num_epochs) \\\n","        + '_l2weight_' + str(L2_WEIGHT) + '_net.pth')\n","if os.path.exists(model_path):\n","    # load pre-trained model and replace its head\n","    if model_type == 'VGG13':\n","        trained_model = models.vgg13(weights=\"IMAGENET1K_V1\")\n","        trained_model.classifier[6] = nn.Linear(4096, len(desired_classes))\n","    else:\n","        trained_model = models.resnet18(weights=\"IMAGENET1K_V1\")\n","        num_ftrs = trained_model.fc.in_features\n","        trained_model.fc = nn.Linear(num_ftrs, len(desired_classes))\n","else:\n","    trained_model = train_model(model, train_loader, test_loader, DEVICE, lr = lr, num_epochs = num_epochs, L2_WEIGHT = L2_WEIGHT)\n","    save_filename = model_type + f'_trained_cifar10_lr_' + str(lr) + '_epochs_' + str(num_epochs) \\\n","        + '_l2weight_' + str(L2_WEIGHT) + '_net.pth'\n","    save_path = os.path.join(os.getcwd(), save_filename)\n","    torch.save(model.state_dict(), save_path)\n","    print('Model saved at: ' + save_path)\n","print('Train model is loaded')\n","\n","# ===========\n","# Get indices of top 'num_test_points' test images with highest test loss\n","# ===========\n","trained_model.eval()\n","test_losses = return_test_losses(test_loader, trained_model)\n","test_idxs = torch.argsort(test_losses, dim=0, descending=True)[:num_test_points]\n","test_idxs = test_idxs.tolist()\n","test_idxs_to_plot = test_idxs[:num_test_point_plot]\n","test_images = [captioned_image(trained_model, filtered_test_dataset[idx], \"test\", idx, test_losses, desired_classes, device=DEVICE) for idx in test_idxs_to_plot]\n","\n","# ===========\n","# Initialize influence module using custom objective\n","# ===========\n","class BinClassObjective(BaseObjective):\n","    def train_outputs(self, model, batch):\n","        return model(batch[0])\n","    def train_loss_on_outputs(self, outputs, batch):\n","        return torch.nn.CrossEntropyLoss()(outputs, batch[1])\n","    def train_regularization(self, params):\n","        return L2_WEIGHT * torch.square(params.norm())\n","    def test_loss(self, model, params, batch):\n","        outputs = model(batch[0])\n","        return torch.nn.CrossEntropyLoss()(outputs, batch[1])\n","# GNH\n","curr_net_gnh = copy.deepcopy(trained_model)\n","lissa_gnh = LiSSAInfluenceModule(model=curr_net_gnh,objective=BinClassObjective(),train_loader=data.DataLoader(filtered_train_dataset, batch_size=32),\n","                                 test_loader=data.DataLoader(filtered_test_dataset, batch_size=32),device=DEVICE,damp=0.001,repeat= repeat_lissa,depth=depth_lissa,\n","                                 scale=scale_lissa,gnh=True)\n","# Newton\n","curr_net_Newton = copy.deepcopy(trained_model)\n","lissa_Newton = LiSSAInfluenceModule(model=curr_net_Newton,objective=BinClassObjective(),train_loader=data.DataLoader(filtered_train_dataset, batch_size=32),\n","                                 test_loader=data.DataLoader(filtered_test_dataset, batch_size=32),device=DEVICE,damp=0.001,repeat= repeat_lissa,depth=depth_lissa,\n","                                 scale=scale_lissa,gnh=False)\n","# ===========\n","# For each test point:\n","#   1. Get the influence scores for a randomly selected subset of training points\n","#   2. Find the most helpful and harmful training points out of this subset\n","# The most helpful point is that which, if removed, most increases the loss at the\n","# test point of interest (as predicted by the influence scores). Conversely, the most harmful\n","# test point is that which most decreases the test loss if removed.\n","# We repeat this process with the Newton-based and the GNH-based influence functions method and compare the time and the results\n","# ===========\n","helpful_images_newton = []\n","harmful_images_newton = []\n","helpful_images_gnh = []\n","harmful_images_gnh = []\n","\n","time_Newton = []\n","time_gnh = []\n","\n","\n","influence_train_indices = list(range(len(filtered_train_dataset)))\n","test_point_ctr = 0\n","\n","for test_idx in tqdm(test_idxs, desc=\"Computing Influences\"):\n","\n","    time_start = time.time()\n","    influences_gnh = lissa_gnh.influences(train_idxs=influence_train_indices, test_idxs=[test_idx])\n","    time_end = time.time()\n","    time_gnh.append(time_end - time_start)\n","    print(f'curr time gnh:{time_end - time_start:.2f}')\n","\n","    time_start = time.time()\n","    influences_hessian = lissa_Newton.influences(train_idxs=influence_train_indices, test_idxs=[test_idx])\n","    time_end = time.time()\n","    time_Newton.append(time_end - time_start)\n","    print(f'curr time Newton:{time_end - time_start:.2f}')\n","\n","    # update images and influences for the points we aim to plot\n","    if test_idx in test_idxs_to_plot:\n","        max_index_gnh = influence_train_indices[np.argmax(influences_gnh)]\n","        max_index_hessian = influence_train_indices[np.argmax(influences_hessian)]\n","        min_index_gnh = influence_train_indices[np.argmin(influences_gnh)]\n","        min_index_hessian = influence_train_indices[np.argmin(influences_hessian)]\n","\n","        helpful_gnh = captioned_image(trained_model, filtered_train_dataset[max_index_gnh], \"train\",\\\n","                influences_gnh.argmax(), influences_gnh, desired_classes, device=DEVICE)\n","        harmful_gnh = captioned_image(trained_model, filtered_train_dataset[min_index_gnh], \"train\",\\\n","                influences_gnh.argmin(), influences_gnh, desired_classes, device=DEVICE)\n","        helpful_images_gnh.append(helpful_gnh)\n","        harmful_images_gnh.append(harmful_gnh)\n","\n","        helpful_newton = captioned_image(trained_model, filtered_train_dataset[max_index_hessian], \"train\",\\\n","                influences_hessian.argmax(), influences_hessian, desired_classes, device=DEVICE)\n","        harmful_newton = captioned_image(trained_model, filtered_train_dataset[min_index_hessian], \"train\",\\\n","                influences_hessian.argmin(), influences_hessian, desired_classes, device=DEVICE)\n","        helpful_images_newton.append(helpful_newton)\n","        harmful_images_newton.append(harmful_newton)\n","\n","    print('======================')\n","    print('Finished test point #' + str(test_idx))\n","    test_point_ctr += 1\n","\n","# save all data to a .pkl file, and plot the histogram\n","image_grid_gnh = [test_images, helpful_images_gnh, harmful_images_gnh]\n","image_grid_Newton = [test_images, helpful_images_newton, harmful_images_newton]\n","\n","print('=======Finished Influence calculation, saving to a file=======')\n","plt.figure(figsize=(10, 6))\n","plt.hist(time_Newton, bins=10, alpha=0.5, label='Hessian, mean: {:.2f}'.format(np.mean(time_Newton)), color='blue', edgecolor='black')\n","plt.hist(time_gnh, bins=10, alpha=0.5, label='Fisher, mean: {:.2f}'.format(np.mean(time_gnh)), color='orange', edgecolor='black')\n","# Add labels and title\n","plt.xlabel('Running Time [sec]', fontweight=\"bold\")\n","plt.ylabel('Frequency', fontweight=\"bold\")\n","plt.title('Histogram of Running Times', fontweight=\"bold\")\n","plt.legend(prop=dict(weight='bold'))\n","plt.savefig('running_times_histogram.pdf')\n","print('Mean Newton:' + str(np.mean(time_Newton)))\n","print('Mean GNH:' + str(np.mean(time_gnh)))\n","print('=======Finished=======')"]},{"cell_type":"markdown","metadata":{"id":"0D1P17QvCayi"},"source":["# Plot image grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZehlyKICb-D"},"outputs":[],"source":["# ===========\n","# Plot image grid\n","# ==========\n","image_grid_gnh = [test_images, helpful_images_gnh, harmful_images_gnh]\n","image_grid_Newton = [test_images, helpful_images_newton, harmful_images_newton]\n","\n","for idx in range(2):\n","    if idx == 0:\n","        curr_image_grid = image_grid_gnh\n","        fig_title = 'analyze_CIFAR10_gnh.pdf'\n","    else:\n","        curr_image_grid = image_grid_Newton\n","        fig_title = 'analyze_CIFAR10_Newton.pdf'\n","\n","    fig, axes = plt.subplots(nrows=3, ncols=num_test_point_plot, sharex=True, sharey=True, figsize=(12, 10))\n","    plt.subplots_adjust(wspace=0.25, hspace=1)\n","\n","    # plot images\n","    if len(axes.shape) == 1:\n","        axes = np.expand_dims(axes, axis=1)\n","    for row_images, row_axes in zip(curr_image_grid, axes):\n","        for (image, caption), ax in zip(row_images, row_axes):\n","            ax.set_title(caption, size=\"large\", weight=\"bold\")\n","            ax.set(aspect=\"equal\")\n","            ax.imshow(image)\n","            ax.set_xticks([])\n","            ax.set_yticks([])\n","    fig.tight_layout()\n","\n","    # write row labels\n","    row_labels = [\"Test Image\", \"Most Helpful\", \"Most Harmful\"]\n","    for ax, label in zip(axes[:, 0], row_labels):\n","        ax.set_ylabel(label, rotation=90, size=\"x-large\", fontweight=\"bold\")\n","    fig.savefig(fig_title, dpi=300)\n","    plt.close(fig)\n","print(\"Finished printing the image grid\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcI2Japi4caZ"},"outputs":[],"source":["# Create times histogram\n","plt.figure(figsize=(10, 6))\n","plt.hist(time_Newton, bins=10, alpha=0.5, label='Hessian, mean: {:.2f}'.format(np.mean(time_Newton)), color='blue', edgecolor='black')\n","plt.hist(time_gnh, bins=10, alpha=0.5, label='Fisher, mean: {:.2f}'.format(np.mean(time_gnh)), color='orange', edgecolor='black')\n","plt.xlabel('Running Time [sec]', fontweight=\"bold\")\n","plt.ylabel('Frequency', fontweight=\"bold\")\n","plt.title('Histogram of Running Times', fontweight=\"bold\")\n","plt.legend(prop=dict(weight='bold'))\n","plt.savefig('running_times_histogram.pdf')\n","print('Mean Newton:' + str(np.mean(time_Newton)))\n","print('Mean GNH:' + str(np.mean(time_gnh)))\n","print('=======Finished=======')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}